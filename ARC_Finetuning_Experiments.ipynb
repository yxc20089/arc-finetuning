{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ARC Fine-tuning Experiments\n\nThis notebook compares different fine-tuning strategies for Qwen2.5-VL on the ARC (AI2 Reasoning Challenge) dataset.\n\n## Experiments:\n1. **Baseline** - Original model without fine-tuning\n2. **Full LoRA** - Fine-tune both vision encoder and language model with LoRA\n3. **Vision-Only LoRA** - Fine-tune only vision encoder with LoRA (freeze LM)\n4. **Vision Full Fine-tune** - Full fine-tuning of vision encoder (no LoRA, uses non-quantized model)\n5. **Language-Only LoRA** - Fine-tune only language model with LoRA (freeze vision)\n6. **Full LoRA OCR** - Fine-tune on OCR task (image → text extraction)\n\n## Output Files:\nAll artifacts are saved locally and automatically copied to Google Drive (when running in Colab):\n- `results/` - JSON results for each experiment\n- `models/` - Saved final model weights\n- `checkpoints/` - Training checkpoints (saved every N steps)\n- `plots/` - Training curves and comparison graphs\n\n**Google Drive path:** `drive/MyDrive/arc_finetuning_outputs/`\n\n**Note:** Existing files on Drive are skipped to avoid overwriting (except results and plots which are always updated)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport json\nimport os\nimport gc\nimport re\nimport shutil\nfrom datetime import datetime\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom unsloth import FastVisionModel\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import TrainerCallback\n\n# Mount Google Drive (Colab only)\nIS_COLAB = \"COLAB_\" in \"\".join(os.environ.keys())\nif IS_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    print(\"Google Drive mounted at /content/drive\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create local output directories\nlocal_dirs = [\"results\", \"models\", \"plots\", \"checkpoints\"]\nfor d in local_dirs:\n    os.makedirs(d, exist_ok=True)\nprint(f\"Local directories created: {', '.join(local_dirs)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Global Configuration\nCONFIG = {\n    \"model_name\": \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",  # 4-bit quantized (for LoRA experiments)\n    \"model_name_full\": \"unsloth/Qwen2.5-VL-7B-Instruct\",      # Full precision (for non-LoRA experiments)\n    \"load_in_4bit\": True,\n    \n    # Data paths (update for Colab: \"drive/MyDrive/arc_train\" etc.)\n    \"train_data_dir\": \"arc_train\",\n    \"val_data_dir\": \"arc_validation\", \n    \"test_data_dir\": \"arc_test\",\n    \n    # Output paths (local)\n    \"results_dir\": \"results\",\n    \"models_dir\": \"models\",\n    \"plots_dir\": \"plots\",\n    \"checkpoints_dir\": \"checkpoints\",\n    \n    # Google Drive output path (all artifacts will be copied here)\n    \"drive_output_dir\": \"drive/MyDrive/arc_finetuning_outputs\",\n    \n    # Training settings\n    \"batch_size\": 2,\n    \"gradient_accumulation_steps\": 4,\n    \"max_steps\": 30,\n    \"learning_rate\": 2e-4,\n    \"warmup_steps\": 5,\n    \"save_steps\": 10,  # Save checkpoint every N steps\n    \n    # LoRA settings\n    \"lora_r\": 16,\n    \"lora_alpha\": 16,\n    \n    # Evaluation\n    \"val_samples\": 50,  # Number of validation samples for quick eval\n    \"seed\": 3407,\n}\n\n# Experiment Configurations\nEXPERIMENTS = {\n    \"baseline\": {\n        \"name\": \"Baseline (No Fine-tuning)\",\n        \"train\": False,\n    },\n    \"full_lora\": {\n        \"name\": \"Full LoRA (Vision + Language)\",\n        \"train\": True,\n        \"use_lora\": True,\n        \"finetune_vision\": True,\n        \"finetune_language\": True,\n        \"task\": \"qa\",\n    },\n    \"vision_lora\": {\n        \"name\": \"Vision-Only LoRA\",\n        \"train\": True,\n        \"use_lora\": True,\n        \"finetune_vision\": True,\n        \"finetune_language\": False,\n        \"task\": \"qa\",\n    },\n    \"vision_full\": {\n        \"name\": \"Vision Full Fine-tune (No LoRA)\",\n        \"train\": True,\n        \"use_lora\": False,  # Full fine-tuning, requires non-quantized model\n        \"finetune_vision\": True,\n        \"finetune_language\": False,\n        \"task\": \"qa\",\n    },\n    \"language_lora\": {\n        \"name\": \"Language-Only LoRA\",\n        \"train\": True,\n        \"use_lora\": True,\n        \"finetune_vision\": False,\n        \"finetune_language\": True,\n        \"task\": \"qa\",\n    },\n    \"full_lora_ocr\": {\n        \"name\": \"Full LoRA OCR Task\",\n        \"train\": True,\n        \"use_lora\": True,\n        \"finetune_vision\": True,\n        \"finetune_language\": True,\n        \"task\": \"ocr\",\n    },\n}\n\n# Create Google Drive directories if in Colab\nif IS_COLAB:\n    drive_base = CONFIG[\"drive_output_dir\"]\n    for d in [\"results\", \"models\", \"plots\", \"checkpoints\"]:\n        os.makedirs(os.path.join(drive_base, d), exist_ok=True)\n    print(f\"Google Drive directories created at: {drive_base}\")\n\nprint(\"Configuration loaded.\")\nprint(f\"\\nExperiments to run: {list(EXPERIMENTS.keys())}\")\nprint(f\"\\nQuantized model: {CONFIG['model_name']}\")\nprint(f\"Full precision model: {CONFIG['model_name_full']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, split_name):\n",
    "    \"\"\"Load dataset from JSONL file.\"\"\"\n",
    "    images_dir = os.path.join(data_dir, f\"{os.path.basename(data_dir)}_images\")\n",
    "    jsonl_file = os.path.join(data_dir, f\"{os.path.basename(data_dir)}.jsonl\")\n",
    "    \n",
    "    dataset = []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            dataset.append({\n",
    "                \"image_path\": os.path.join(images_dir, item[\"image_path\"]),\n",
    "                \"answer_key\": item[\"answer_key\"],\n",
    "                \"id\": item[\"id\"],\n",
    "                \"question\": item[\"question\"],\n",
    "                \"choices\": item[\"choices\"],\n",
    "            })\n",
    "    \n",
    "    print(f\"Loaded {len(dataset)} {split_name} samples\")\n",
    "    return dataset\n",
    "\n",
    "# Load all datasets\n",
    "train_data = load_dataset(CONFIG[\"train_data_dir\"], \"training\")\n",
    "val_data = load_dataset(CONFIG[\"val_data_dir\"], \"validation\")\n",
    "test_data = load_dataset(CONFIG[\"test_data_dir\"], \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_text(item):\n",
    "    \"\"\"Format question with choices as text (for OCR task target).\"\"\"\n",
    "    choices_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(item[\"choices\"])])\n",
    "    return f\"{item['question']}\\n{choices_text}\"\n",
    "\n",
    "def convert_to_qa_format(sample):\n",
    "    \"\"\"Convert sample to QA task format (image → answer letter).\"\"\"\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"<answer>{sample['answer_key']}</answer>\"}]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def convert_to_ocr_format(sample):\n",
    "    \"\"\"Convert sample to OCR task format (image → question text).\"\"\"\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    target_text = format_question_text(sample)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": target_text}]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(\"Data conversion functions defined.\")\n",
    "print(f\"\\nSample QA target: <answer>{train_data[0]['answer_key']}</answer>\")\n",
    "print(f\"\\nSample OCR target:\\n{format_question_text(train_data[0])[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTracker(TrainerCallback):\n",
    "    \"\"\"Callback to track training losses.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.steps = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            self.steps.append(state.global_step)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return {\"steps\": self.steps, \"losses\": self.losses}\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"Extract answer letter from model output.\"\"\"\n",
    "    match = re.search(r'<answer>\\s*([A-Da-d])\\s*</answer>', text)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    match = re.search(r'(?:^|answer is|answer:|choice)\\s*([A-Da-d])\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=50, use_adapters=True):\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    FastVisionModel.for_inference(model)\n",
    "    \n",
    "    # Setup adapter context\n",
    "    if use_adapters:\n",
    "        context = nullcontext()\n",
    "    else:\n",
    "        try:\n",
    "            from peft import disable_adapter_layers\n",
    "            context = disable_adapter_layers(model)\n",
    "        except:\n",
    "            context = nullcontext()\n",
    "    \n",
    "    results = []\n",
    "    correct = 0\n",
    "    \n",
    "    with context:\n",
    "        for sample in tqdm(dataset[:num_samples], desc=\"Evaluating\"):\n",
    "            image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "            \n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}]}]\n",
    "            input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "            inputs = tokenizer(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model.generate(**inputs, max_new_tokens=64, use_cache=True, \n",
    "                                       temperature=0.1, do_sample=False)\n",
    "            \n",
    "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response.lower() else response\n",
    "            \n",
    "            predicted = extract_answer(response)\n",
    "            expected = sample[\"answer_key\"]\n",
    "            is_correct = predicted == expected\n",
    "            \n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            results.append({\n",
    "                \"id\": sample[\"id\"],\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": predicted,\n",
    "                \"correct\": is_correct,\n",
    "            })\n",
    "    \n",
    "    accuracy = correct / len(results) * 100\n",
    "    return results, accuracy\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def copy_to_drive(local_path, drive_subdir, skip_if_exists=True):\n    \"\"\"Copy file or directory to Google Drive.\n    \n    Args:\n        local_path: Local file or directory path\n        drive_subdir: Subdirectory in drive output (e.g., 'results', 'models')\n        skip_if_exists: If True, skip copying if destination exists\n    \"\"\"\n    if not IS_COLAB:\n        return  # Skip if not in Colab\n    \n    drive_base = CONFIG[\"drive_output_dir\"]\n    filename = os.path.basename(local_path)\n    drive_path = os.path.join(drive_base, drive_subdir, filename)\n    \n    # Check if already exists\n    if skip_if_exists and os.path.exists(drive_path):\n        print(f\"  [SKIP] Already exists on Drive: {drive_path}\")\n        return False\n    \n    # Copy file or directory\n    try:\n        if os.path.isdir(local_path):\n            if os.path.exists(drive_path):\n                shutil.rmtree(drive_path)\n            shutil.copytree(local_path, drive_path)\n        else:\n            shutil.copy2(local_path, drive_path)\n        print(f\"  [COPIED] {local_path} -> {drive_path}\")\n        return True\n    except Exception as e:\n        print(f\"  [ERROR] Failed to copy {local_path}: {e}\")\n        return False\n\ndef save_results(exp_name, results_dict, save_dir=\"results\"):\n    \"\"\"Save experiment results to JSON and copy to Drive.\"\"\"\n    filepath = os.path.join(save_dir, f\"{exp_name}_results.json\")\n    with open(filepath, \"w\") as f:\n        json.dump(results_dict, f, indent=2)\n    print(f\"Results saved to {filepath}\")\n    copy_to_drive(filepath, \"results\", skip_if_exists=False)  # Always update results\n\ndef save_model(model, tokenizer, exp_name, save_dir=\"models\"):\n    \"\"\"Save model weights and copy to Drive.\"\"\"\n    model_dir = os.path.join(save_dir, exp_name)\n    model.save_pretrained(model_dir)\n    tokenizer.save_pretrained(model_dir)\n    print(f\"Model saved to {model_dir}\")\n    copy_to_drive(model_dir, \"models\", skip_if_exists=True)\n\ndef save_plot(fig, filename, save_dir=\"plots\"):\n    \"\"\"Save plot and copy to Drive.\"\"\"\n    filepath = os.path.join(save_dir, filename)\n    fig.savefig(filepath, dpi=150, bbox_inches='tight')\n    print(f\"Plot saved to {filepath}\")\n    copy_to_drive(filepath, \"plots\", skip_if_exists=False)  # Always update plots\n\ndef copy_checkpoints_to_drive(exp_name, checkpoint_dir):\n    \"\"\"Copy training checkpoints to Drive.\"\"\"\n    if not IS_COLAB or not os.path.exists(checkpoint_dir):\n        return\n    \n    drive_checkpoint_dir = os.path.join(CONFIG[\"drive_output_dir\"], \"checkpoints\", exp_name)\n    os.makedirs(drive_checkpoint_dir, exist_ok=True)\n    \n    for item in os.listdir(checkpoint_dir):\n        if item.startswith(\"checkpoint-\"):\n            local_ckpt = os.path.join(checkpoint_dir, item)\n            drive_ckpt = os.path.join(drive_checkpoint_dir, item)\n            if os.path.exists(drive_ckpt):\n                print(f\"  [SKIP] Checkpoint exists: {item}\")\n            else:\n                shutil.copytree(local_ckpt, drive_ckpt)\n                print(f\"  [COPIED] Checkpoint: {item}\")\n\ndef load_fresh_model(use_quantized=True):\n    \"\"\"Load a fresh copy of the base model.\n    \n    Args:\n        use_quantized: If True, load 4-bit quantized model (for LoRA).\n                      If False, load full precision model (for full fine-tuning).\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    if use_quantized:\n        model_name = CONFIG[\"model_name\"]\n        load_in_4bit = True\n        print(f\"Loading quantized model: {model_name}\")\n    else:\n        model_name = CONFIG[\"model_name_full\"]\n        load_in_4bit = False\n        print(f\"Loading full precision model: {model_name}\")\n    \n    model, tokenizer = FastVisionModel.from_pretrained(\n        model_name,\n        load_in_4bit=load_in_4bit,\n        use_gradient_checkpointing=\"unsloth\",\n    )\n    return model, tokenizer\n\ndef cleanup_model(model, trainer=None):\n    \"\"\"Clean up model from memory.\"\"\"\n    if trainer is not None:\n        del trainer\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(\"Save/Load functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_existing_results(exp_key):\n    \"\"\"Check if results already exist locally or on Drive.\"\"\"\n    local_path = f\"results/{exp_key}_results.json\"\n    \n    # Check local\n    if os.path.exists(local_path):\n        print(f\"[SKIP] Results already exist: {local_path}\")\n        with open(local_path, \"r\") as f:\n            return json.load(f)\n    \n    # Check Drive\n    if IS_COLAB:\n        drive_path = os.path.join(CONFIG[\"drive_output_dir\"], \"results\", f\"{exp_key}_results.json\")\n        if os.path.exists(drive_path):\n            print(f\"[SKIP] Results already exist on Drive: {drive_path}\")\n            # Copy from Drive to local\n            shutil.copy2(drive_path, local_path)\n            with open(local_path, \"r\") as f:\n                return json.load(f)\n    \n    return None\n\ndef run_experiment(exp_key, exp_config, train_data, val_data, test_data):\n    \"\"\"Run a single experiment and return results.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"EXPERIMENT: {exp_config['name']}\")\n    print(f\"{'='*60}\")\n    \n    # Check if results already exist\n    existing_results = check_existing_results(exp_key)\n    if existing_results is not None:\n        return existing_results\n    \n    # Determine if we need quantized or full precision model\n    use_lora = exp_config.get(\"use_lora\", True)\n    use_quantized = use_lora  # Use quantized model only for LoRA experiments\n    \n    # Load fresh model\n    model, tokenizer = load_fresh_model(use_quantized=use_quantized)\n    \n    results = {\n        \"name\": exp_config[\"name\"],\n        \"config\": exp_config,\n        \"timestamp\": datetime.now().isoformat(),\n        \"model_type\": \"quantized\" if use_quantized else \"full_precision\",\n    }\n    \n    loss_tracker = None\n    trainer = None\n    checkpoint_dir = f\"checkpoints/{exp_key}\"\n    \n    if exp_config.get(\"train\", False):\n        if use_lora:\n            # Configure model with LoRA adapters\n            model = FastVisionModel.get_peft_model(\n                model,\n                finetune_vision_layers=exp_config[\"finetune_vision\"],\n                finetune_language_layers=exp_config[\"finetune_language\"],\n                finetune_attention_modules=True,\n                finetune_mlp_modules=True,\n                r=CONFIG[\"lora_r\"],\n                lora_alpha=CONFIG[\"lora_alpha\"],\n                lora_dropout=0,\n                bias=\"none\",\n                random_state=CONFIG[\"seed\"],\n            )\n        else:\n            # Full fine-tuning: unfreeze specific layers (non-quantized model)\n            trainable_params = 0\n            total_params = 0\n            for name, param in model.named_parameters():\n                total_params += param.numel()\n                if exp_config[\"finetune_vision\"] and \"visual\" in name.lower():\n                    param.requires_grad = True\n                    trainable_params += param.numel()\n                elif exp_config[\"finetune_language\"] and \"visual\" not in name.lower():\n                    param.requires_grad = True\n                    trainable_params += param.numel()\n                else:\n                    param.requires_grad = False\n            print(f\"Trainable params: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n        \n        # Prepare dataset based on task\n        task = exp_config.get(\"task\", \"qa\")\n        if task == \"qa\":\n            converted_data = [convert_to_qa_format(s) for s in tqdm(train_data, desc=\"Converting to QA\")]\n        else:  # ocr\n            converted_data = [convert_to_ocr_format(s) for s in tqdm(train_data, desc=\"Converting to OCR\")]\n        \n        # Setup trainer with checkpoint saving\n        loss_tracker = LossTracker()\n        FastVisionModel.for_training(model)\n        \n        trainer = SFTTrainer(\n            model=model,\n            tokenizer=tokenizer,\n            data_collator=UnslothVisionDataCollator(model, tokenizer),\n            train_dataset=converted_data,\n            callbacks=[loss_tracker],\n            args=SFTConfig(\n                per_device_train_batch_size=CONFIG[\"batch_size\"],\n                gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n                warmup_steps=CONFIG[\"warmup_steps\"],\n                max_steps=CONFIG[\"max_steps\"],\n                learning_rate=CONFIG[\"learning_rate\"],\n                logging_steps=1,\n                optim=\"adamw_8bit\",\n                weight_decay=0.001,\n                lr_scheduler_type=\"linear\",\n                seed=CONFIG[\"seed\"],\n                output_dir=checkpoint_dir,\n                save_strategy=\"steps\",\n                save_steps=CONFIG[\"save_steps\"],\n                save_total_limit=3,  # Keep only last 3 checkpoints\n                report_to=\"none\",\n                remove_unused_columns=False,\n                dataset_text_field=\"\",\n                dataset_kwargs={\"skip_prepare_dataset\": True},\n                max_length=2048,\n            ),\n        )\n        \n        # Train\n        print(f\"\\nTraining {exp_config['name']}...\")\n        train_result = trainer.train()\n        results[\"training\"] = {\n            \"runtime_seconds\": train_result.metrics[\"train_runtime\"],\n            \"loss_history\": loss_tracker.get_data(),\n        }\n        print(f\"Training completed in {train_result.metrics['train_runtime']:.2f}s\")\n        \n        # Copy checkpoints to Drive\n        print(\"Copying checkpoints to Drive...\")\n        copy_checkpoints_to_drive(exp_key, checkpoint_dir)\n        \n        # Save final model\n        save_model(model, tokenizer, exp_key)\n    \n    # Evaluate on validation\n    print(f\"\\nEvaluating on validation set ({CONFIG['val_samples']} samples)...\")\n    val_results, val_accuracy = evaluate_model(\n        model, tokenizer, val_data, \n        num_samples=CONFIG[\"val_samples\"], \n        use_adapters=use_lora and exp_config.get(\"train\", False)\n    )\n    results[\"validation\"] = {\"accuracy\": val_accuracy, \"num_samples\": len(val_results)}\n    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n    \n    # Evaluate on test\n    print(f\"\\nEvaluating on test set ({len(test_data)} samples)...\")\n    test_results, test_accuracy = evaluate_model(\n        model, tokenizer, test_data, \n        num_samples=len(test_data), \n        use_adapters=use_lora and exp_config.get(\"train\", False)\n    )\n    results[\"test\"] = {\"accuracy\": test_accuracy, \"num_samples\": len(test_results)}\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n    \n    # Save results\n    save_results(exp_key, results)\n    \n    # Cleanup\n    cleanup_model(model, trainer)\n    \n    return results\n\nprint(\"Experiment runner defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "ALL_RESULTS = {}\n",
    "\n",
    "# Run each experiment\n",
    "for exp_key, exp_config in EXPERIMENTS.items():\n",
    "    ALL_RESULTS[exp_key] = run_experiment(exp_key, exp_config, train_data, val_data, test_data)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{'Experiment':<40} {'Val Acc':<12} {'Test Acc':<12} {'Val Impr':<12} {'Test Impr':<12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "baseline_val = ALL_RESULTS[\"baseline\"][\"validation\"][\"accuracy\"]\n",
    "baseline_test = ALL_RESULTS[\"baseline\"][\"test\"][\"accuracy\"]\n",
    "\n",
    "for exp_key, results in ALL_RESULTS.items():\n",
    "    val_acc = results[\"validation\"][\"accuracy\"]\n",
    "    test_acc = results[\"test\"][\"accuracy\"]\n",
    "    val_impr = val_acc - baseline_val if exp_key != \"baseline\" else 0\n",
    "    test_impr = test_acc - baseline_test if exp_key != \"baseline\" else 0\n",
    "    \n",
    "    impr_str_val = f\"{val_impr:+.2f}%\" if exp_key != \"baseline\" else \"--\"\n",
    "    impr_str_test = f\"{test_impr:+.2f}%\" if exp_key != \"baseline\" else \"--\"\n",
    "    \n",
    "    print(f\"{results['name']:<40} {val_acc:>10.2f}%  {test_acc:>10.2f}%  {impr_str_val:>10}  {impr_str_test:>10}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save comprehensive results\n",
    "save_results(\"comprehensive_comparison\", ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualizations\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nexp_names = [ALL_RESULTS[k][\"name\"] for k in ALL_RESULTS.keys()]\nexp_names_short = [\"Baseline\", \"Full\\nLoRA\", \"Vision\\nLoRA\", \"Vision\\nFull\", \"Lang\\nLoRA\", \"OCR\\nLoRA\"]\nval_accs = [ALL_RESULTS[k][\"validation\"][\"accuracy\"] for k in ALL_RESULTS.keys()]\ntest_accs = [ALL_RESULTS[k][\"test\"][\"accuracy\"] for k in ALL_RESULTS.keys()]\n\n# 1. Bar chart - All accuracies\nax1 = axes[0, 0]\nx = np.arange(len(exp_names_short))\nwidth = 0.35\nbars1 = ax1.bar(x - width/2, val_accs, width, label='Validation', color='#3498db', edgecolor='black')\nbars2 = ax1.bar(x + width/2, test_accs, width, label='Test', color='#e74c3c', edgecolor='black')\nax1.set_ylabel('Accuracy (%)', fontsize=12)\nax1.set_title('Accuracy by Experiment', fontsize=14, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(exp_names_short, fontsize=9)\nax1.legend()\nax1.set_ylim(0, 100)\n\n# 2. Improvement over baseline\nax2 = axes[0, 1]\nexp_keys_train = [k for k in ALL_RESULTS.keys() if k != \"baseline\"]\nnames_train = [exp_names_short[i+1] for i in range(len(exp_keys_train))]\nval_imprs = [ALL_RESULTS[k][\"validation\"][\"accuracy\"] - baseline_val for k in exp_keys_train]\ntest_imprs = [ALL_RESULTS[k][\"test\"][\"accuracy\"] - baseline_test for k in exp_keys_train]\n\nx2 = np.arange(len(names_train))\nbars1 = ax2.bar(x2 - width/2, val_imprs, width, label='Validation', color='#2ecc71', edgecolor='black')\nbars2 = ax2.bar(x2 + width/2, test_imprs, width, label='Test', color='#9b59b6', edgecolor='black')\nax2.set_ylabel('Improvement (%)', fontsize=12)\nax2.set_title('Improvement over Baseline', fontsize=14, fontweight='bold')\nax2.set_xticks(x2)\nax2.set_xticklabels(names_train, fontsize=9)\nax2.legend()\nax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\n# 3. Training curves\nax3 = axes[1, 0]\ncolors = plt.cm.tab10(np.linspace(0, 1, len(exp_keys_train)))\nfor i, exp_key in enumerate(exp_keys_train):\n    if \"training\" in ALL_RESULTS[exp_key] and ALL_RESULTS[exp_key][\"training\"][\"loss_history\"][\"losses\"]:\n        losses = ALL_RESULTS[exp_key][\"training\"][\"loss_history\"][\"losses\"]\n        steps = ALL_RESULTS[exp_key][\"training\"][\"loss_history\"][\"steps\"]\n        ax3.plot(steps, losses, '-o', label=names_train[i], markersize=3, color=colors[i])\nax3.set_xlabel('Training Step', fontsize=12)\nax3.set_ylabel('Loss', fontsize=12)\nax3.set_title('Training Loss Curves', fontsize=14, fontweight='bold')\nax3.legend(fontsize=8)\nax3.grid(True, alpha=0.3)\n\n# 4. Test accuracy ranking\nax4 = axes[1, 1]\nsorted_results = sorted(ALL_RESULTS.items(), key=lambda x: x[1][\"test\"][\"accuracy\"], reverse=True)\nsorted_names = [ALL_RESULTS[k][\"name\"][:25] for k, _ in sorted_results]\nsorted_accs = [r[\"test\"][\"accuracy\"] for _, r in sorted_results]\ncolors_rank = ['#2ecc71' if i == 0 else '#3498db' if acc > baseline_test else '#e74c3c' for i, acc in enumerate(sorted_accs)]\nax4.barh(range(len(sorted_names)), sorted_accs, color=colors_rank, edgecolor='black')\nax4.set_yticks(range(len(sorted_names)))\nax4.set_yticklabels(sorted_names, fontsize=9)\nax4.set_xlabel('Test Accuracy (%)', fontsize=12)\nax4.set_title('Test Accuracy Ranking', fontsize=14, fontweight='bold')\nax4.axvline(x=baseline_test, color='red', linestyle='--', label=f'Baseline ({baseline_test:.1f}%)')\nfor i, acc in enumerate(sorted_accs):\n    ax4.text(acc + 0.5, i, f'{acc:.1f}%', va='center', fontsize=9)\n\nplt.tight_layout()\n\n# Save plot locally and to Drive\nsave_plot(fig, 'comprehensive_comparison.png')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*70)\n\n# Find best performing experiment\nbest_exp = max([(k, v[\"test\"][\"accuracy\"]) for k, v in ALL_RESULTS.items() if k != \"baseline\"], key=lambda x: x[1])\nworst_exp = min([(k, v[\"test\"][\"accuracy\"]) for k, v in ALL_RESULTS.items() if k != \"baseline\"], key=lambda x: x[1])\n\nprint(f\"\\nBaseline Test Accuracy: {baseline_test:.2f}%\")\nprint(f\"\\nBest Performing: {ALL_RESULTS[best_exp[0]]['name']}\")\nprint(f\"  - Test Accuracy: {best_exp[1]:.2f}%\")\nprint(f\"  - Improvement: {best_exp[1] - baseline_test:+.2f}%\")\n\nprint(f\"\\nWorst Performing: {ALL_RESULTS[worst_exp[0]]['name']}\")\nprint(f\"  - Test Accuracy: {worst_exp[1]:.2f}%\")\nprint(f\"  - Improvement: {worst_exp[1] - baseline_test:+.2f}%\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OUTPUT LOCATIONS:\")\nprint(\"=\"*70)\nprint(\"\\nLocal directories:\")\nprint(\"  - results/     : JSON results for each experiment\")\nprint(\"  - models/      : Final model weights\")\nprint(\"  - checkpoints/ : Training checkpoints\")\nprint(\"  - plots/       : Visualization plots\")\n\nif IS_COLAB:\n    print(f\"\\nGoogle Drive (persistent storage):\")\n    print(f\"  {CONFIG['drive_output_dir']}/\")\n    print(f\"    ├── results/     : JSON results\")\n    print(f\"    ├── models/      : Final model weights\")\n    print(f\"    ├── checkpoints/ : Training checkpoints\")\n    print(f\"    └── plots/       : Visualization plots\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}